{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression\n",
    "=====\n",
    "\n",
    "Linear Regression attempts to predict a continuous outcome feature (**Y**) from one or more explanatory features (**X**).  \n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1 X$$\n",
    "\n",
    "$\\beta_0$ is called the intercept term, and represents the expected mean value of Y when all explanatory features equal 0.  \n",
    "$\\beta_1$ is called a beta coefficient, and represents the expected change in the value of Y that results from a one unit change in X.  \n",
    "\n",
    "\n",
    "Below is an example of a linear regression with only one explanatory feature. The red dots indicate the actual data, and the blue line represents the predicted **Y** values based on the provided **X** values.  $\\beta_0$ appears to equals 0, and $\\beta_1$ appears to equal 2.\n",
    "<img src=\"./images/LinearRegression.png\" alt=\"Go Find Missing Image\" style=\"width: 500px;height=500\"/>\n",
    "\n",
    "In this lab, we will attempt to construct a linear regression in order to answer a question that Kiva borrowers may have: \n",
    "\n",
    "**What impacts the loan amount requested? **\n",
    "\n",
    "To ensure that our linear regressor is appropriate and interpretable, we will have to confirm the following assumptions are not violated:\n",
    "  1. Linear relationship between x and y\n",
    "  2. Normality\n",
    "  3. Minimal multicollinearity (if multiple variables)\n",
    "  4. No autocorrelation \n",
    "  5. Homoscedasticity \n",
    "  - Additional rule of thumb: at least 20 observations per independent variable in the analysis\n",
    "  \n",
    "If these assumptions are violated, then the predictive power of the linear regression is still valid but the information concerning the most important features is not. It is important to keep this in mind!\n",
    "\n",
    "\n",
    "Here's a look ahead at what we'll be doing in these series of notebooks: \n",
    "\n",
    "3.1 Load Data and Build Univariate Linear Regression        \n",
    "3.2 Check Assumptions  \n",
    "3.3 Build Multivariate Linear Regression \n",
    "3.4 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import packages\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Packages for checking assumptions\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set()\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# Packages for checking assumptions\n",
    "from scipy import stats as stats\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "# Set jupyter notebook preferences\n",
    "# the command below means that the output of multiple commands in a cell will be output at once.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# the command below tells jupyter to display up to 100 columns, this keeps everything visible\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data_path = '../data/'\n",
    "df = pd.read_csv(data_path+'clean_data.csv.zip', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Univariate Linear Regression\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test split\n",
    "\n",
    "Prior to building our model, we first need to split our dataset into a training set and a test set.  We will use our training set to train our regressor, and we will use our test set for model validation.\n",
    "To achieve this, we will use call sklearn's [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), and set the input parameter `test_size` to .2 so that 20% of the data will be assigned to the test set and 80% of the data will be assigned to the training set.\n",
    "\n",
    "**We set the test set aside and only look at this at the end to evaluate the models performance on unseen data.**\n",
    "\n",
    "We fix the random state so that each time we run the train_test_split code, we get the same distribution of data. This is important as keeping the data split constant allows us to compare results from different sessions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our dependant variable\n",
    "y = df['loan_amount']\n",
    "# Define  our independant variables\n",
    "X = df[['borrower_count']]\n",
    "# Add an intercept term to the independant variables. This is needed in order to include the constant term from\n",
    "# linear regression equation.\n",
    "X['cnst'] = 1\n",
    "# Split our data into training and test data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Linear Regression\n",
    "In order to build our linear regressor, we will use [statsmodels](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html) implementation.  The are other implementations out there; however, we will use statmodels because it creates a nice summary table for model evaluation. Let's print out the summary table to demonstrate how easy it is to train the model and see the results.\n",
    "\n",
    "For an in-depth review on all the statistics and numbers given in the summary below, check out this [awesome page!](http://connor-johnson.com/2014/02/18/linear-regression-with-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = sm.OLS(endog = y_train,exog = X_train).fit()\n",
    "print(model0.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model interpretation\n",
    "-----\n",
    "\n",
    "This is the typical regression output. It's a lot to digest! Remember that our simple model can be represented as a linear equation, like this: \n",
    "\n",
    "    loan_amount = intercept + coef*borrower_count\n",
    "\n",
    "Therefore, these results can be translated as follows: \n",
    "\n",
    "    loan_amount = 397 + 50*borrower_count\n",
    "    \n",
    "Let's state the results. Regardless of the number of borrowers, the predicted loan amount is 384 dollars. However, with the every additional borrower, the loan amount increases by 50 dollars. This is consistent with our earlier explanatory analysis. How do we know however, whether this is a significant result?\n",
    "\n",
    "We have a sufficient amount of confidence in this conclusion because the **p-value** is reported to be 0.000. In technical terms, the p-value is **the probability of getting results as extreme as the ones observed given no correlation. **\n",
    "\n",
    "In statistics, we want our results to fall within the 95% confidence interval, or the p-value to be <= 0.05. This means, \"[i]f repeated samples were taken and the 95% confidence interval was computed for each sample, 95% of the intervals would contain the population mean. A 95% confidence interval has a 0.95 probability of containing the population mean. 95% of the population distribution is contained in the confidence interval.\" [Read more here.](http://www.investopedia.com/terms/s/standard-error.asp) The p-value is an indicator of where we fall in the confidence interval. In English, small p-value (<= 0.05) indicates strong evidence that the coefficient is different than 0. \n",
    "\n",
    "This is a relatively simplified explanation of p-values. Don't worry if it's not immediately intuitive - [not even professional statisticians can easily explain this concept.](http://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/) To get a deeper understanding, we recommend grabbing the nearest textbook on statistics to review! \n",
    "\n",
    "Note also that the Adjusted R Squared is extremely low. To recap lessons from Module 2, the Adjusted R Squared is an explanation of how much of the outcome feature can be explained by the model's explanatory features. This low Adj R Squared suggests that the predictive value of borrower_count alone is pretty low - it cannot be used as the single feature to predict loan_amount. \n",
    "\n",
    "This result is in line with our understanding of the world - there must be other factors influencing loan_amount!\n",
    "\n",
    "**Before we start looking for other explanatory features, it is important to note that interpretation of the results above is only valid if the assumptions of linear regression are not violated. Lets go through these assumptions now...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Linear Regression: Check Assumptions\n",
    "=====\n",
    "\n",
    "In order for linear regression to be an appropriate model, the following assumptions must be confirmed:\n",
    "\n",
    "1. Linear relationship between x and y\n",
    "1. Normality of residuals\n",
    "1. Minimal multicollinearity \n",
    "1. No autocorrelation\n",
    "1. Homoscedasticity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity\n",
    "First, as the name implies, linear regression requires the underlying relationship between the independent (x) and dependent (y) variables to be linear. This means that a line drawn through the points can capture the relationship, with some degree of error. \n",
    "\n",
    "We must also be aware of outliers, which can throw off a linear relationship. \n",
    "\n",
    "The most straightforward check of linearity is to make scatter plots of the variables chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x='borrower_count', y='loan_amount', data=df, fit_reg=False)\n",
    "ax.set_title('Scatter plot of borrower_count vs requested loan amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_correlate = df[['loan_amount','posted_year', 'borrower_count']]\n",
    "g = sns.pairplot(features_to_correlate,diag_kind='kde')\n",
    "g.fig.subplots_adjust(wspace=.02, hspace=.02)\n",
    "g.fig.set_size_inches(10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plot, it's apparent that there is a  general linear relationship betweeen the borrower count and the loan amount requested, however there seems to be more than one trend. We will note this and investigate it further on in this notebook!\n",
    "\n",
    "We can use a scatter matrix to look at the scatter plots for numerous features at the same time. Below, we use the scatter matrix plotting function that comes with the pandas library.\n",
    "\n",
    "The diagonals of the scatter matrix represent the 'kernal density approximation' for the given features. This can be thought of as a continuous histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality\n",
    "\n",
    "Normality means the distribution of values falls into a normal distribution. \n",
    "\n",
    "The results summary printed above gives us an indication of whether or not our data is normally distributed. If the **Prob(JB)** number is less than 0.05, this means that we can reject the Null hypothesis that the data distribution is normal. If this is the case, we need to look into our model more closely see where this is coming from and how/if we can rectify the issue.\n",
    "\n",
    "We can visualize the normality of the data by making histograms or Q-Q (quantile-quantile) plots.\n",
    "\n",
    "First we will look at a histogram for the dependant variable, loan_amount. From this, we can see that the dependant variable is skewed to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df[df['loan_amount']<=6000]['loan_amount'], bins=range(0, 6000, 100), kde=False, norm_hist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can create a quantile-quantile plot to see how the distribution of data compares that expected from a normal distribution. In these plots, the quantiles from the measured data are compared to that of a normal distribution and if the measured data is normal, we expect to see a straight line.\n",
    "\n",
    "The plot below clearly shows that the depedant varaible is not normal and therefore we need to investigate the skew further to see if the data required transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert depedant variable to a numpy and standardise the data\n",
    "y = np.asarray(df['loan_amount'])\n",
    "standardised_y = (y-np.mean(y))/np.std(y)\n",
    "# create a normal Q-Q plot using the stats module probplot\n",
    "plt.figure(figsize=(12,8))\n",
    "stats.probplot(standardised_y, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Normal Q-Q plot\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a variable is not normal, it may be adjusted by a [non-linear transformation](https://stats.stackexchange.com/questions/298/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-va).\n",
    "\n",
    "We already know our data is skewed by outliers, however before we begin modeling let's quantify the degree to which it is skewed.\n",
    "\n",
    "[Scipy's stats package](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.stats.skew.html) allows you to conduct a skew test. A normally distributed dataset would return about 0. A skewness value of greater than 0 means there are more loans in the left tail of the distribution. Typically, a skew score of +-5 is deemed acceptable.\n",
    "\n",
    "\n",
    "[Multivariate normality](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) is a generalization of one-dimensional, or univariate, normality, to more dimensions. For a regression using multiple independent (x) variables, this means checking that all of the linear combinations of variables used are normal. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{stats.skew(df.loan_amount, axis=None):.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms our suspicion that the data is skewed to the left, i.e. the mean is smaller than the median. However, the degree to which it is skewed is not troubling. +-5 is fairly reasonable. \n",
    "\n",
    "We can continue with the linear regression model for predicting the loan_amount however, we should be aware that our beta-coefficients may be not be reliable do to the skewedness of the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you face a dataset with more severely skewed data, we describe a way to address this in the appendix below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "\n",
    "To recap Module 2's lesson, multicollinearity means that two or more explanatory features are highly correlated.\n",
    "\n",
    "This makes it difficult for the model to try to differentiate between each feature's influence on the explanatory feature.\n",
    "\n",
    "Again, the summary table from the linear regression can help us again. There is a statistic in the bottom right hand corner known as the **condition number**. This indicates the severity and impact of multicolinearity in the regression and as a rule fo thumb, values over 30 are an issue. \n",
    "\n",
    "One way to check this is by examining the correlation matrix and making sure none of the variables correlate to 1. Numpy's Corrcoef function returns a matrix of Pearson correlation coefficients. \n",
    "\n",
    "Read more about Numpy's Corrcoef [here](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.corrcoef.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would take a long time for us to test each and every relationship between two sets of variables, so let's create a correlation matrix that lets us visualize every correlation in a dataset using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_tbl = df[['loan_amount','partner_loans_posted', 'posted_month', 'partner_delinquency_rate']]\n",
    "df_corr_tbl.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a correlation heatmap to visualise the correlation between many features at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the correlation matrix\n",
    "relevant_colums = ['loan_amount', 'partner_loans_posted','partner_delinquency_rate', 'posted_year','posted_month', 'num_tags','children_int','borrower_count','days_to_fund','age_int']\n",
    "correlation_matrix  = df[relevant_colums].corr()\n",
    "sns.heatmap(correlation_matrix, cmap = \"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot tell us a few interesting things:\n",
    "\n",
    "**Correlation with loan_amount**\n",
    "The following variable have the highest correlation coefficients with loan_amount\n",
    "- borrower_count\n",
    "- partner_loans_posted\n",
    "- num tags\n",
    "- days_to_fund\n",
    "\n",
    "** Multicolinearity**\n",
    "Out of the variables above, there are potetnial multicolinearity issues with:\n",
    "- Borrower_count is correlated with num_tags. \n",
    "- Partner_loans_posted is inversely correlated with partner_loans_posted\n",
    "\n",
    "However, in all cases the multicollinearity is weak (correlation coefficient less than 0.6 and therefore it is not a major cause of concern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No autocorrelation\n",
    "\n",
    "A variable has autocorrelation when values in the series correlate with others in the same series, separated by a given interval.\n",
    "\n",
    "In terms of the residuals, this means checking that residuals are independent.\n",
    "\n",
    "A check for this is the **Durbin-Watson test**. The durbin-watson statistic ranges from 0 to 4 with values around 2 suggesting no autocorrelation. Values < 2 suggest positive correlation and values > 2 suggest negative correlation.\n",
    "\n",
    "In the summary table above, the Durbin-Watson test has a value of 1.998 so we can be happy that there is no autocorrelation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homoscedasticity\n",
    "\n",
    "Homoscedasticity is a statistical term that means that the spread of the residuals are equal across the regression line. This assumption means that the variance around the regression line is the same for all values of the predictor variable (X). \n",
    "\n",
    "Again, the summary table from the linear regression allows us to evaluate whether the model has homoscedasticity. If **Prob(Omnibus)** number is less than 0.05 we can reject the Null Hypothesis that the residuals are distributed uniformly and normally around zero.\n",
    "\n",
    "In the case above, the Prob(Omnibus) is 0.00 which clearly indicates that the model does not have homoscedasticity.\n",
    "\n",
    "This can be seen the plot below, the distribution of the residual are not consistent across the range of data. Therefore, we need to work on our regression model more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model0.predict(X_train)\n",
    "ax = sns.regplot(x=y_pred, y=(y_train-y_pred), fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "To recap the finding from the linear regression assumptions check above:\n",
    "\n",
    "1. Linear relationship between x and y\n",
    "  - For the independant variable 'borrower_count', there is multi-trend linear relationship\n",
    "1. Normality\n",
    "  - The dependant variable data is not normally distributed and is left-hand skewed. \n",
    "1. Minimal multicollinearity \n",
    "  - There is minimal/weak multicollinearity between feature which is not much of a concern.\n",
    "1. No autocorrelation\n",
    "  - Durbin-Watson test indicates no autocorrelation\n",
    "1. Homoscedasticity \n",
    "  - With the single independant feature of borrower_count, the model is not homoscedacity. We will build upon our model and try again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
